# encoding:utf-8'''iacr.org 爬虫'''import requestsimport urllibimport refrom bs4 import BeautifulSoupweburl = 'https://eprint.iacr.org/complete/'a=input("how many latest papers do you want to download : ")print("正在连接",weburl)page=requests.get(weburl).textprint("连接成功")soup=BeautifulSoup(page,'lxml')dt_list=soup.find_all('dt')num=1#print(dt_list)for tag_dt in dt_list:    print("tag_dt",tag_dt)    tag_a=tag_dt.find('a',text=re.compile('.*?[p|P][d|D][f|F].*?'))    print("tag_a",tag_a)    if tag_a:        if num>int(a):            break        #tag_dd=tag_a.parent.next_sibling.next_sibling        tag_dd=tag_a.parent.next_sibling        #print(tag_dd,"\n",tag_a,"\n")        #filename=tag_dd.find('b',        filename=tag_dd.get_text().strip().replace(' ','_').replace('\n','_')+'.pdf'        print('下载文件%d：'%num,filename)        #tag_dt=tag_div.parent.previous_sibling.previous_sibling        #tag_a=tag_dt.find('a',attrs={'href':re.compile('(/pdf/.+?)')})        pdf_url='https://eprint.iacr.org/'+tag_a.attrs['href']        print(pdf_url)        urllib.request.urlretrieve(pdf_url, filename)                #r = requests.get(pdf_url)         #with open(filename, "wb") as f:            #f.write(r.content)        #print(filename,'下载结束')        num+=1                                 #print htmlcode# reg = r'<a href="(/pdf/.+?)" title'# reg_pdf = re.compile(reg)# pdf_list_all = reg_pdf.findall(htmlcode)# pdf_list = []# n=0# for i in range(a) :#     pdf_list.append(pdf_list_all[n])#     n += 1# n=0# pdf_name = []# for i in pdf_list:#     pdf_name_tmp=pdf_list[n]#     pdf_name_tmp=pdf_name_tmp.replace(".","_")#     pdf_name_tmp=pdf_name_tmp.replace("/pdf/","")#     pdf_name.append(pdf_name_tmp)#     n += 1# x=0# for pdf in pdf_list[0:a]:#     pdf_url = 'https://arxiv.org' + pdf + '.pdf'#     print ('downloading %s \n' %pdf_name[x])#     urllib.urlretrieve(pdf_url, '%s.pdf' %pdf_name[x])#     print ('finish downloading NO.%d\n\n'%(x+1))#     x += 1